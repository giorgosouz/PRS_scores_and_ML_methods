# -*- coding: utf-8 -*-
"""MunnebSNPs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ekukdAqFXTbukFY0-Yc80WK7-iACQLi3
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from google.colab import drive
drive.mount('/content/drive/',force_remount=True)
# %cd drive/MyDrive/genetics

from __future__ import absolute_import, division, print_function
import argparse
from tensorflow.keras import Model, layers
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from numpy import genfromtxt
from sklearn import svm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import warnings
warnings.filterwarnings('ignore')
from datetime import datetime
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
scaler = StandardScaler()
import tensorflow.keras
from sklearn.metrics import precision_score, recall_score, accuracy_score
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Activation,ActivityRegularization
from tensorflow.keras.utils import to_categorical
from sklearn import metrics
import tensorflow as tf
from tensorflow.keras.layers import LSTM, GRU,Bidirectional,Conv1D,Reshape,InputLayer
from sklearn import preprocessing
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
import seaborn as sn
import matplotlib.pyplot as plt
from pylab import rcParams
import sys
import os
from sklearn import tree, ensemble
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score


def metric(name,best_model,x_train,y_train,x_test,y_test):
    y_pred = best_model.predict(x_test)
    sn.set(font_scale=2)
    rcParams['figure.figsize'] = 7, 7
    confusion_matrix = pd.crosstab(y_test.argmax(axis=1), y_pred.argmax(axis=1),rownames=['Actual'], colnames=['Predicted'])
    sn.heatmap(confusion_matrix, annot=True)
    plt.savefig(args.path+os.sep+name+"Test.png")
    plt.clf()
    confusion_matrix = pd.crosstab(y_train.argmax(axis=1), best_model.predict(x_train).argmax(axis=1), rownames=['Actual'], colnames=['Predicted'])
    sn.heatmap(confusion_matrix, annot=True)
    plt.savefig(args.path+os.sep+name+"Train.png")
    plt.clf()
    
    
    
def plotting(history):
    fig = plt.figure()
    history_dict = history.history
    print(history_dict.keys())
    plt.subplot(2,1,1)
    plt.plot(history_dict['accuracy'])
    plt.plot(history_dict['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['Training Set', 'Validation Set'], loc='lower right')
    plt.subplot(2,1,2)
    plt.plot( history_dict['loss'])
    plt.plot( history_dict['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['Training Set', 'Validation Set'], loc='upper right')
    plt.tight_layout()
    
    
    
import pandas as pd
from os import makedirs

def model1(name,activationfunction=["sigmoid"],dropout=[5],optimizer = ["Adam"],batch_size =[5],epochs=[30],l1=[0.01],l2=[0],validationsplit=[0.3]):
    for activation in activationfunction:
        for drop in dropout:
            for opt in optimizer:
                for b in batch_size:
                    for e in epochs:
                        for v in validationsplit:
                            model = Sequential()
                            model.add(Dense(100, input_shape=(x_train.shape[1],)))
                            model.add(Activation(activation))
                            model.add(Dropout(drop))
                            model.add(Dense(50))
                            model.add(Activation(activation))
                            model.add(Dropout(drop))
                            model.add(Dense(1))
                            model.add(Activation("sigmoid"))
                            model.summary()
                            tf.keras.utils.plot_model(model, to_file='model1.pdf', show_shapes=True,show_layer_activations=True)
                            ll=tf.keras.losses.BinaryCrossentropy()
                            model.compile(loss=ll, metrics=['Accuracy'],optimizer=opt)
                            history = model.fit(X_train, Y_train,batch_size=b, epochs=e,validation_split=v,verbose=2,callbacks=callback)
                            loss, acc = model.evaluate(X_test, Y_test)
                            Y_pred = model.predict(X_test)
                            print(str(activation),str(drop),str(opt),str(b),str(e),str(v))
                            print("train AUC",roc_auc_score(Y_train, model.predict(X_train)))
                            print("test AUC",roc_auc_score(Y_test, model.predict(X_test)))
                            print(history.history.keys())
                            plt.plot(history.history['auc'])
                            plt.plot(history.history['val_auc'])
                            plt.title('model auc')
                            plt.ylabel('auc')
                            plt.xlabel('epoch')
                            plt.legend(['train', 'test'], loc='upper left')
                            plt.show()
                            # summarize history for loss
                            plt.plot(history.history['loss'])
                            plt.plot(history.history['val_loss'])
                            plt.title('model loss')
                            plt.ylabel('loss')
                            plt.xlabel('epoch')
                            plt.legend(['train', 'test'], loc='upper left')
                            plt.show()
                            return np.amax(Y_pred,axis=1)





def model2(name,activationfunction=["sigmoid"],dropout=[0.2],optimizer = ["Adam"],batch_size =[5],epochs=[30],l1=[0.01],l2=[0],validationsplit=[0.3]):
    for activation in activationfunction:
        for drop in dropout:
            for opt in optimizer:
                for b in batch_size:
                    for e in epochs:
                        for v in validationsplit:
                            model = Sequential()
                            model.add(LSTM(5, input_shape=(X_train.shape[1],1), return_sequences=True))                            
                          
                            # model.add(LSTM(3,return_sequences=True))
                            model.add(Flatten())
                            model.add(Dense(50))
                            model.add(Activation(activation))
                            model.add(Dropout(drop))
                            model.add(Dense(1))
                            model.add(Activation('sigmoid'))
                            model.summary()
                            tf.keras.utils.plot_model(model, to_file='model2.pdf', show_shapes=True,show_layer_activations=True)
                            model.compile(loss='binary_crossentropy', metrics=['Accuracy'],optimizer=opt)
                            history = model.fit(X_train, Y_train,batch_size=b, epochs=e,validation_split=v,verbose=2,callbacks=callback)
                            loss, acc = model.evaluate(X_test, Y_test)
                            Y_pred = model.predict(X_test)
                            print(str(activation),str(drop),str(opt),str(b),str(e),str(v))
                            print("train AUC",roc_auc_score(Y_train, model.predict(X_train)))
                            print("test AUC",roc_auc_score(Y_test, model.predict(X_test)))
                            print(history.history.keys())
                            plt.plot(history.history['auc'])
                            plt.plot(history.history['val_auc'])
                            plt.title('model auc')
                            plt.ylabel('auc')
                            plt.xlabel('epoch')
                            plt.legend(['train', 'test'], loc='upper left')
                            plt.show()
                            # summarize history for loss
                            plt.plot(history.history['loss'])
                            plt.plot(history.history['val_loss'])
                            plt.title('model loss')
                            plt.ylabel('loss')
                            plt.xlabel('epoch')
                            plt.legend(['train', 'test'], loc='upper left')
                            plt.show()
                            return np.amax(Y_pred,axis=1)




def model3(name,activationfunction=["sigmoid"],dropout=[0.2],optimizer = ["Adam"],batch_size =[5],epochs=[30],l1=[0.01],l2=[0],validationsplit=[0.3]):
    for activation in activationfunction:
        for drop in dropout:
            for opt in optimizer:
                for b in batch_size:
                    for e in epochs:
                        for v in validationsplit:
                            model = Sequential()
                            model.add(Conv1D(5,20, input_shape=(X_train.shape[1],1)))                            
                            model.add(Flatten())
                            model.add(Dense(50))
                            model.add(Activation(activation))
                            model.add(Dropout(drop))
                            model.add(Dense(1))
                            model.add(Activation('sigmoid'))
                            model.summary()
                            tf.keras.utils.plot_model(model, to_file='model3.pdf', show_shapes=True,show_layer_activations=True)
                            model.compile(loss='binary_crossentropy', metrics=['Accuracy'],optimizer=opt)
                            history = model.fit(X_train, Y_train,batch_size=b, epochs=e,validation_split=v,verbose=2,callbacks=callback)
                            loss, acc = model.evaluate(X_test, Y_test)
                            Y_pred = model.predict(X_test)
                            print(str(activation),str(drop),str(opt),str(b),str(e),str(v))
                            print("train AUC",roc_auc_score(Y_train, model.predict(X_train)))
                            print("test AUC",roc_auc_score(Y_test, model.predict(X_test)))
                            print(history.history.keys())
                            plt.plot(history.history['auc'])
                            plt.plot(history.history['val_auc'])
                            plt.title('model auc')
                            plt.ylabel('auc')
                            plt.xlabel('epoch')
                            plt.legend(['train', 'test'], loc='upper left')
                            plt.show()
                            # summarize history for loss
                            plt.plot(history.history['loss'])
                            plt.plot(history.history['val_loss'])
                            plt.title('model loss')
                            plt.ylabel('loss')
                            plt.xlabel('epoch')
                            plt.legend(['train', 'test'], loc='upper left')
                            plt.show()
                            return np.amax(Y_pred,axis=1)

def model4(name,activationfunction=["sigmoid"],dropout=[5],optimizer = ["Adam"],batch_size =[5],epochs=[30],l1=[0.01],l2=[0],validationsplit=[0.3]):
    for activation in activationfunction:
        for drop in dropout:
            for opt in optimizer:
                for b in batch_size:
                    for e in epochs:
                        for v in validationsplit:
                            model = Sequential()
                            model.add(Dense(100, input_shape=(x_train.shape[1],)))
                            model.add(Activation(activation))
                            model.add(Dropout(drop))
                            model.add(Dense(50))
                            model.add(Activation(activation))
                            model.add(Dropout(drop))
                            model.add(Dense(1))
                            model.add(Activation("sigmoid"))
                            model.summary()
                            ll=tf.keras.losses.BinaryCrossentropy(from_logits=True)
                            model.compile(loss=ll, metrics=['accuracy'],optimizer=opt)
                            history = model.fit(X_train, Y_train,batch_size=b, epochs=e,validation_split=v,callbacks=callback,verbose=2)
                            loss, acc = model.evaluate(X_test, Y_test)
                            model.pop()
                            Y_pred = model.predict(X_test)
                            print(str(activation),str(drop),str(opt),str(b),str(e),str(v))
                            # print("train AUC",roc_auc_score(Y_train, model.predict(X_train)))
                            # print("test AUC",roc_auc_score(Y_test, model.predict(X_test)))
                            print(history.history.keys())
                            plt.plot(history.history['accuracy'])
                            plt.plot(history.history['val_accuracy'])
                            plt.title('Model accuracy')
                            plt.ylabel('Accuracy')
                            plt.xlabel('epoch')
                            plt.legend(['train', 'test'], loc='upper left')
                            plt.show()
                            # summarize history for loss
                            plt.plot(history.history['loss'])
                            plt.plot(history.history['val_loss'])
                            plt.title('Model loss')
                            plt.ylabel('loss')
                            plt.xlabel('epoch')
                            plt.legend(['train', 'test'], loc='upper left')
                            plt.show()
                            return np.amax(Y_pred,axis=1)


                        
                        
                        
direc = os.getcwd()
trainpath= direc+os.sep+"train/"
testpath = direc+os.sep+"test/"
allsnps = os.listdir(direc)

for files in allsnps:
    #print(files)
    if "_snps" not in files and "pv_5e-40" in files:
        x_train = pd.read_csv(direc+os.sep+files+os.sep+'ptrain.raw', sep="\s+")
        x_test = pd.read_csv(direc+os.sep+files+os.sep+'ptest.raw', sep="\s+")
        y_train = pd.read_csv(trainpath+'YRI.pheno', sep="\s+")
        y_test= pd.read_csv(testpath+'YRI.pheno', sep="\s+")
        x_train =x_train.iloc[:,6:].values
        x_test =x_test.iloc[:,6:].values
        y_train =y_train.iloc[:,2:].values
        y_test =y_test.iloc[:,2:].values
        scaler = StandardScaler()
        std_scale = preprocessing.StandardScaler().fit(x_train)
        x_train = std_scale.transform(x_train)
        x_test = std_scale.transform(x_test)
        X_train = x_train
        X_test = x_test
        Y_train = y_train
        Y_test = y_test
        activationFunctions = ["relu"]
        ddropout = [0.3]
        optimizer = ["Adam"]
        batchsize = [254]
        epochsnumber = [100]
        validation = [0.3]
        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30,verbose=1,restore_best_weights=True),tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5,patience=15)]
        data = model3("ANN1",activationFunctions,ddropout,optimizer,batchsize,epochsnumber,l1=[0],l2=[0],validationsplit=validation)
        x = pd.DataFrame()
        x['0'] = data
        x.to_csv(direc+os.sep+files+os.sep+"ML_probability",header=False, index=False)

np.count_nonzero(y_test==0)

import pandas as pd
import sys
import os

import pandas as pd
import os
import glob
import numpy as np
import shutil
import sys
import numpy as np
import scipy as sp
from scipy import stats
import matplotlib.pyplot as plt
import os
import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score
import math
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
import math

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
import statistics
import seaborn as sns
direc = sys.argv[1]

###QC-basefile
def NormalizeData(data):
    return (data - np.min(data)) / (np.max(data) - np.min(data))
def calculateAUCPRScice(direc):
  best = pd.read_csv(direc+os.sep+"result"+os.sep+"PRScice_PRS.best",sep="\s+")
  pheno = pd.read_csv(direc+os.sep+"/test/YRI.pheno",sep="\s+")
  #plt.hist(best["PRS"].values)
  #plt.show()

  #best["PRS"].values[best["PRS"] >= sum(best["PRS"].values)/len(best["PRS"].values)] = 1
  #best["PRS"].values[best["PRS"] < sum(best["PRS"].values)/len(best["PRS"].values)] = 0
  
  #plt.plot(,best["PRS"].values)
  #plt.show()
  temp = NormalizeData(best["PRS"].values)
  best["PRS"] = NormalizeData(np.array(best["PRS"].values))
  
  best["PRS"].values[best["PRS"] >=0.5] = 1
  best["PRS"].values[best["PRS"] < 0.5] = 0

  
  print("AUC",roc_auc_score(np.array(best["PRS"].values), np.array(pheno['phenotype'].values)))
  a= best["PRS"].values
  b = pheno['phenotype'].values
  accuracy = len([a[i] for i in range(0, len(a)) if a[i] == b[i]]) / len(a)
  print("Accuracy",accuracy)
  return temp


def calculateAUCPlink(direc):
  pheno = pd.read_csv(direc+os.sep+"/test/YRI.pheno",sep="\s+")
  files = os.listdir(direc+os.sep+"files")
  maxxacc=0
  maxauc = 0
  profile = ""
  temp = []
  for loop in files:

    if ".profile" in loop:      
      best = pd.read_csv(direc+os.sep+"files"+os.sep+loop,sep="\s+")
      #plt.hist(best["PRS"].values)
      #plt.show()
    
      #best["PRS"].values[best["PRS"] >= sum(best["PRS"].values)/len(best["PRS"].values)] = 1
      #best["PRS"].values[best["PRS"] < sum(best["PRS"].values)/len(best["PRS"].values)] = 0



      best["SCORE"] = NormalizeData(best["SCORE"].values)
    
      best["SCORE"].values[best["SCORE"] >=0.5] = 1
      best["SCORE"].values[best["SCORE"] < 0.5] = 0
      
      if maxauc<roc_auc_score(best["SCORE"].values, pheno['phenotype'].values):
        maxauc = roc_auc_score(best["SCORE"].values, pheno['phenotype'].values)
        temp = NormalizeData(pd.read_csv(direc+os.sep+"files"+os.sep+loop,sep="\s+")["SCORE"].values)
        profile = loop
        a= best["SCORE"].values
        b = pheno['phenotype'].values
        accuracy = len([a[i] for i in range(0, len(a)) if a[i] == b[i]]) / len(a)
        maxxacc = accuracy
  print("AUC",maxauc)
  print("Accuracy",maxxacc)
  print("Profile", profile)
  return temp

def calculateAUClasso(direc):
  best = pd.read_csv(direc+os.sep+"result"+os.sep+"test.txt",sep="\s+",header=None)
  pheno = pd.read_csv(direc+os.sep+"/test/YRI.pheno",sep="\s+")
  #plt.hist(best[0].values)
  #plt.show()

  #best["PRS"].values[best["PRS"] >= sum(best["PRS"].values)/len(best["PRS"].values)] = 1
  #best["PRS"].values[best["PRS"] < sum(best["PRS"].values)/len(best["PRS"].values)] = 0
  best[0] = NormalizeData(best[0].values)
  temp = NormalizeData(best[0].values)
  best[0].values[best[0] >=0.5] = 1
  best[0].values[best[0] < 0.5] = 0

  print("AUC",roc_auc_score(best[0].values, pheno['phenotype'].values))
  a= best[0].values
  b = pheno['phenotype'].values
  accuracy = len([a[i] for i in range(0, len(a)) if a[i] == b[i]]) / len(a)
  print("Accuracy",accuracy)
  return temp

def calculateAUCMachineLearning(direc):
  pheno = pd.read_csv(direc+os.sep+"/test/YRI.pheno",sep="\s+")
  
  files = os.listdir(direc)
  maxxacc=0
  maxauc = 0
  profile = ""
  temp = []
  for loop in files:

    if "pv_5e-40" in loop:      
      best = pd.read_csv(direc+os.sep+loop+os.sep+"ML_probability",sep="\s+",header=None)
      #plt.hist(best[0].values)
      #plt.show()
    
      #best["PRS"].values[best["PRS"] >= sum(best["PRS"].values)/len(best["PRS"].values)] = 1
      #best["PRS"].values[best["PRS"] < sum(best["PRS"].values)/len(best["PRS"].values)] = 0
      best[0] = NormalizeData(best[0].values)
    
      best[0].values[best[0] >=0.5] = 1
      best[0].values[best[0] < 0.5] = 0
      
      if maxauc<roc_auc_score(best[0].values, pheno['phenotype'].values):
        maxauc = roc_auc_score(best[0].values, pheno['phenotype'].values)
        temp = NormalizeData(pd.read_csv(direc+os.sep+loop+os.sep+"ML_probability",sep="\s+",header=None)[0].values)
        profile = loop
        a= best[0].values
        b = pheno['phenotype'].values
        accuracy = len([a[i] for i in range(0, len(a)) if a[i] == b[i]]) / len(a)
        maxxacc = accuracy
  print("AUC",maxauc)
  print("Accuracy",maxxacc)
  #print("Profile", profile)
  return temp
  

direc = os.getcwd()

df=pd.DataFrame()

print('PRS')
x = calculateAUCPRScice(direc)
df['PRSice']=x
n_samples = len(x)
rng = np.random.RandomState(0)
x = sns.displot(x,color='green')




print('Plink')
x = calculateAUCPlink(direc)
df['Plink']=x
x = sns.displot(x,color = 'purple')


print('lasso')
x = calculateAUClasso(direc)
df['lassosum']=x
x = sns.displot(x,color='black')



print('ML')
x = calculateAUCMachineLearning(direc)
df['ML']=x

x = sns.displot(x,color='yellow')




x.figure.legend(labels=['PRScice','Plink','Lassosum','Machine Learning'])



x.figure.savefig(direc+os.sep+"result"+os.sep+"output.png")

x=sns.displot(df)
x.figure.savefig(direc+os.sep+"result"+os.sep+"output.png")

from sklearn.metrics import accuracy_score as rr

rr(best[0].values, pheno['phenotype'].values)

tim=pd.DataFrame(columns=np.arange(0.1,0.9,0.1),index=('lasso','PRSice','ML'))


for thr in np.arange(0.1,0.9,0.1):

  best = pd.read_csv(direc+os.sep+"result"+os.sep+"test.txt",sep="\s+",header=None)
  pheno = pd.read_csv(direc+os.sep+"/test/YRI.pheno",sep="\s+")

  best[0] = NormalizeData(best[0].values)
  temp = NormalizeData(best[0].values)


  best[0].values[best[0] >= thr] = 1
  best[0].values[best[0] < thr] = 0
  print(thr)
  print("AUC",rr(best[0].values, pheno['phenotype'].values))
  tim.loc['lasso',thr]=rr(best[0].values, pheno['phenotype'].values)



  best = pd.read_csv(direc+os.sep+"result"+os.sep+"PRScice_PRS.best",sep="\s+")
  pheno = pd.read_csv(direc+os.sep+"/test/YRI.pheno",sep="\s+")

  temp = NormalizeData(best["PRS"].values)
  best["PRS"] = NormalizeData(np.array(best["PRS"].values))
 
  print(thr)
  best["PRS"].values[best["PRS"] >=thr] = 1
  best["PRS"].values[best["PRS"] < thr] = 0

  
  print("AUC",rr(np.array(best["PRS"].values), np.array(pheno['phenotype'].values)))
  tim.loc['PRSice',thr]=rr(np.array(best["PRS"].values), np.array(pheno['phenotype'].values))
######ML

  pheno = pd.read_csv(direc+os.sep+"/test/YRI.pheno",sep="\s+")
  
  files = os.listdir(direc)
  maxxacc=0
  maxauc = 0
  profile = ""
  temp = []
  for loop in files:

    if "pv_5e-40" in loop:      
      best = pd.read_csv(direc+os.sep+loop+os.sep+"ML_probability",sep="\s+",header=None)
      #plt.hist(best[0].values)
      #plt.show()
    
      #best["PRS"].values[best["PRS"] >= sum(best["PRS"].values)/len(best["PRS"].values)] = 1
      #best["PRS"].values[best["PRS"] < sum(best["PRS"].values)/len(best["PRS"].values)] = 0
      best[0] = NormalizeData(best[0].values)
    
      best[0].values[best[0] >=thr] = 1
      best[0].values[best[0] < thr] = 0
      print(thr)
      print("AUC",rr(best[0].values, pheno['phenotype'].values))
      tim.loc['ML',thr]=rr(best[0].values, pheno['phenotype'].values)




tim

pheno = pd.read_csv(direc+os.sep+"/test/YRI.pheno",sep="\s+")['phenotype'].values
x = sns.displot(pheno,color='yellow')

f=df.drop(columns='Plink')
sns.displot(f,legend=True,palette=('red','green','blue'),kde=True)
plt.xlabel('Propability')
plt.ylabel('Count')
plt.savefig('ind.png',dpi=500,bbox_inches='tight')

!pip install visualkeras
import visualkeras
visualkeras.layered_view(model1)

activation='relu'
drop=0.3
model = Sequential()
model.add(Dense(100, input_shape=(x_train.shape[1],)))
model.add(Activation(activation))
model.add(Dropout(drop))
model.add(Dense(50))
model.add(Activation(activation))
model.add(Dropout(drop))
model.add(Dense(1))
model.add(Activation("sigmoid"))



import visualkeras
visualkeras.layered_view(model,legend=True)

from ann_visualizer.visualize import ann_viz
ann_viz(model, view=True, filename='cconstruct_model', title='CNN — Model 1 — Simple Architecture')

model = Sequential()
model.add(LSTM(5, input_shape=(X_train.shape[1],1)))                            

# model.add(LSTM(3,return_sequences=True))
model.add(Flatten())
model.add(Dense(50))
model.add(Activation(activation))
model.add(Dropout(drop))
model.add(Dense(1))
model.add(Activation('sigmoid'))

visualkeras.layered_view(model,legend=True)

model = Sequential()
model.add(Conv1D(5,20, input_shape=(X_train.shape[1],1)))                            
model.add(Flatten())
model.add(Dense(50))
model.add(Activation(activation))
model.add(Dropout(drop))
model.add(Dense(1))
model.add(Activation('sigmoid'))

visualkeras.layered_view(model,legend=True)